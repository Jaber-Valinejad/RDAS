# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10h9Fg0RO5qLQwUakZNbtzvaNZDib7sLn
"""

import pandas as pd
# Read the CSV file into a Pandas DataFrame
#Gard = pd.read_csv('/content/exporttttt.csv')
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
import re
import json
import ast
from itertools import permutations
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModel
import torch

# Sample text
#text = "The goal of tis project was ird. This aim is not to go the first sentence. This is not the second sentence? And this is the third sentence."
#check_sen(text)

def split_sentence(sentence):
    # Use regular expression to split words without including punctuation
    words = re.findall(r'\b\w+\b', sentence)
    return words
def word_matching(text,word):
   for i in  split_sentence(word):
     if i not in text:
        return False
   return True

def get_gard_title(text, list_chcek):
  if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']: text1=stem_text(text.lower())
  elif list_chcek in [ 'Synonyms_sw_nltk']  :          text1=remove_stop_words(text.lower())
  else:                                                  text1=text.lower()
  text2=split_sentence(text1)
  out=dict()
  for i in Gard.index:
    if Gard[list_chcek][i] != []:
      for j in  Gard[list_chcek][i]:
         if j in text1 and word_matching(text2,j)==True:
           if Gard['GardName'][i] in out:
                if len(j.split()) ==1:   out[Gard['GardName'][i]][0]+=text2.count(j)
                else: out[Gard['GardName'][i]][0]+=text1.count(j)
           else:
                if len(j.split()) ==1:out[Gard['GardName'][i]]=[text2.count(j)]
                else:  out[Gard['GardName'][i]]=[text1.count(j)]
  if out== {}: return None
  return out

def get_gard_title_stem_exact(text):
    exact_matching=get_gard_title(text, 'Synonyms_sw_bow')
    #print(exact_matching)
    Stemming_chcek=get_gard_title(text, 'Synonyms_sw_stem_bow')
    #print(Stemming_chcek)
    if exact_matching is None:
        exact_matching = {}
    if Stemming_chcek is None:
        Stemming_chcek = {}
    combined_dict = {}
    combined_dict.update(exact_matching)
    combined_dict.update(Stemming_chcek)
    # Remove keys that are part of another key
    keys_to_remove = set()
    for key1 in combined_dict:
        for key2 in combined_dict:
            if key1 != key2 and key1 in key2:
                keys_to_remove.add(key1)
    for key in keys_to_remove:
        del combined_dict[key]
    if combined_dict=={}:return None
    for key1 in combined_dict:
        combined_dict[key1]=1
    return combined_dict

import spacy
# Load spaCy model with sentencizer component
nlp = spacy.load("en_core_web_sm")
# Function to determine verb tense
def get_verb_tense(verb):
    if "VBD" in verb.tag_:
        return "past"
    elif ("MD" in verb.tag_ and "will" in verb.lemma_.lower()) or ('aim' in verb.lemma_.lower() ) :
        return "future"
    elif "VBP" in verb.tag_ or "VBZ" in verb.tag_:
        return "present"
    else:
        return "unknown"
# Function to determine if a sentence is negated
def is_sentence_negated(sentence):
    for token in sentence:
        if token.dep_ == "neg":
            return True
    return False


def check_sen(text):
  # Process the text
  doc = nlp(text)
  # Iterate over sentences in the document
  first_sentence = ''
  Priority,Future_positive,present_positive,positive='','','',''
  for i, sent in enumerate(doc.sents, 1):
    # Initialize a set to store unique tenses in the sentence
    sentence_tenses = set()
    # Iterate over tokens in the sentence
    for token in sent:
        # Check if the token is a verb
        if token.pos == spacy.symbols.VERB or token.pos == spacy.symbols.AUX:
            # Check the tense of the verb
            tense = get_verb_tense(token)
            sentence_tenses.add(tense)

    # Determine the overall tense of the sentence
    if is_sentence_negated(sent)==False and  ("past" not in sentence_tenses):
        if i == 1:    first_sentence = sent.text
        #positive+=sent.text
        elif  ("the goal of" in sent.text.lower()) or ("aim" in sent.text.lower()):
           Priority+=sent.text
        elif "future" in sentence_tenses:
           Future_positive+=sent.text
        elif "present" in sentence_tenses and is_sentence_negated(sent)==False:
           present_positive+=sent.text
        if i == 1:    first_sentence = sent.text
  return first_sentence,Priority,Future_positive,present_positive 


def get_sentence_with_word(paragraph, target_word):
    if not isinstance(paragraph, str):
        return ''

    # Define characters indicating the start of a new sentence
    new_sentence_chars = ['-', ':', ';', '1)', '2)', '3)', '4)', '5)', '6)', '7)', '8)']

    # Split the paragraph into sentences using provided characters
    for char in new_sentence_chars:
        paragraph = paragraph.replace(char, '.')

    # Split the paragraph into sentences using standard punctuation
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', paragraph)

    # Check for the target word in each sentence
    sen=''
    for sentence in sentences:
        if target_word.lower() in sentence.lower():
            sen+= sentence
    return sen


def stem_text_finding(text):
    # Initialize the Porter Stemmer
    stemmer = PorterStemmer()
    # Remove punctuation
    text_without_punctuation = text
    # Tokenize the text into words
    words = word_tokenize(text_without_punctuation)
    # Perform stemming on each word
    stemmed_words = [stemmer.stem(word) for word in words]
    # Join the stemmed words back into a single string
    stemmed_text = ' '.join(stemmed_words)
    return stemmed_text

def split_sentence(sentence):
    # Use regular expression to split words without including punctuation
    words = re.findall(r'\b\w+\b', sentence)
    return words
def word_matching(text,word):
   for i in  split_sentence(word):
     if i not in text:
        return False
   return True

def get_gard_abstract(text, list_chcek):
  #text=check_sen(text)
  if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']: text1=stem_text(text.lower())
  elif list_chcek in [ 'Synonyms_sw_nltk']  :          text1=remove_stop_words(text.lower())
  else:                                                  text1=text.lower()
  text2=split_sentence(text1)
  out=dict()
  sen=dict()
  for i in Gard.index:
    if Gard[list_chcek][i] != []:
      for j in  Gard[list_chcek][i]:
         if j in text1 and word_matching(text2,j)==True:
           if Gard['GardName'][i] in out:
                if len(j.split()) ==1:
                   out[Gard['GardName'][i]][0]+=text2.count(j)
                   if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] += get_sentence_with_word(stem_text_finding(text.lower()), j)
                   else:    sen[Gard['GardName'][i]] += get_sentence_with_word(text1, j)
                else:
                   out[Gard['GardName'][i]][0]+=text1.count(j)
                   if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] += get_sentence_with_word(stem_text_finding(text.lower()), j)
                   else:    sen[Gard['GardName'][i]] += get_sentence_with_word(text1, j)
           else:
                if len(j.split()) ==1:
                     out[Gard['GardName'][i]]=[text2.count(j)]
                     if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] = get_sentence_with_word(stem_text_finding(text.lower()), j)
                     else:    sen[Gard['GardName'][i]] = get_sentence_with_word(text1, j)
                else:
                     out[Gard['GardName'][i]]=[text1.count(j)]
                     if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] = get_sentence_with_word(stem_text_finding(text.lower()), j)
                     else:    sen[Gard['GardName'][i]] = get_sentence_with_word(text1, j)
  if out== {}: return None,None
  return out,sen

def combine_dictionaries_count(dict1, dict2):
    combined_dict = {}
    # Update combined_dict with values from dict1
    for key, value in dict1.items():
        combined_dict[key] = combined_dict.get(key, 0) + sum(value)
    # Update combined_dict with values from dict2
    for key, value in dict2.items():
        combined_dict[key] = combined_dict.get(key, 0) + sum(value)
    return combined_dict

def combine_dictionaries_sent(dict1, dict2):
    combined_dict = {}
    # Update combined_dict with values from dict1
    for key, value in dict1.items():
        if key in combined_dict:
            combined_dict[key] += value
        else:
            combined_dict[key] = value
    # Update combined_dict with values from dict2
    for key, value in dict2.items():
        if key in combined_dict:
            combined_dict[key] += value
        else:
            combined_dict[key] = value
    return combined_dict

def  modified_dict(combined_dict,combined_dict_sen):
    keys_to_remove = set()
    for key1 in combined_dict:
        for key2 in combined_dict:
          #try:
            if key1 != key2 and (key1 in key2) and (combined_dict[key1] <= combined_dict[key2]) and (combined_dict_sen[key1] in combined_dict_sen[key2]):
                keys_to_remove.add(key1)
          #except:
          #  pass
    for key in keys_to_remove:
        del combined_dict[key]
        del combined_dict_sen[key]
    return combined_dict


def get_gard_abstract_stem_exact(text):
  if text and isinstance(text, str):
    exact_matching, exact_matching_sen=get_gard_abstract(text, 'Synonyms_sw')
    #print(exact_matching)
    Stemming_chcek, Stemming_chcek_sen=get_gard_abstract(text, 'Synonyms_sw_stem')
    #print(Stemming_chcek)
    if exact_matching is None:exact_matching = {}
    if Stemming_chcek is None:Stemming_chcek = {}
    if exact_matching_sen is None:exact_matching_sen = {}
    if Stemming_chcek_sen is None:Stemming_chcek_sen = {}

    combined_dict    = combine_dictionaries_count(exact_matching,Stemming_chcek)
    combined_dict_sen= combine_dictionaries_sent(exact_matching_sen,Stemming_chcek_sen)
    # Remove keys that are part of another key
    combined_dict=modified_dict(combined_dict,combined_dict_sen)
    if combined_dict=={}:return {}
    return combined_dict
  return {}


def is_about_term(input_text, target_term):
    # Load ClinicalBERT model and tokenizer
    model_name = "emilyalsentzer/Bio_ClinicalBERT"
    # ClinicalBERT: "emilyalsentzer/Bio_ClinicalBERT"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    # Tokenize input text and target term
    input_tokens = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    term_tokens = tokenizer(target_term, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Get embeddings from ClinicalBERT
    with torch.no_grad():
        input_embedding = model(**input_tokens).last_hidden_state.mean(dim=1)
        term_embedding = model(**term_tokens).last_hidden_state.mean(dim=1)

    # Calculate cosine similarity between text and term
    similarity = util.pytorch_cos_sim(input_embedding, term_embedding)

    # Define a threshold for similarity
    similarity_threshold = 0.7

    # Return True if similarity is above the threshold, indicating the text is about the term
    return similarity.item() #> similarity_threshold


def normalize_combined_dictionary(input_text,dict1, dict2, dict3, dict4,min_, max_):
    dict1 = {key: value * 4 for key, value in dict1.items()}
    # Make the values of the second dictionary two times
    dict2 = {key: value * 3 for key, value in dict2.items()}
    dict3 = {key: value * 2 for key, value in dict3.items()}
    # Combine all dictionaries
    combined_dict = {key: dict1.get(key, 0) + dict2.get(key, 0) + dict3.get(key, 0) + dict4.get(key, 0) for key in set(dict1) | set(dict2) | set(dict3) | set(dict4)}
    # Normalize the values of the combined dictionary
    total_frequency = sum(combined_dict.values())

    # Check if total_frequency is zero to avoid division by zero
    if total_frequency == 0:
        return {}
    normalized_dict = {key: min_ + (max_ - min_) * (value / total_frequency) for key, value in combined_dict.items()}
    
    result_dict = {}
    for key, value in normalized_dict.items():
      if  is_about_term(input_text.lower(), key) >=0.7:
        result_dict[key] = [value, is_about_term(input_text.lower(), key)]
    return result_dict


def update_dictionary(dictionary):
    updated_dict = {}
    for key, value in dictionary.items():
        new_key = Gard[Gard['GardName'] == key]['GardId'].tolist()
        if new_key:
            new_key = new_key[0].replace('"', '')
            updated_dict[(key,new_key)] = value
        else:
            updated_dict[key] = value
    return updated_dict



def grad_id(title_, Public_health_relevance_statement, abstract_):
    if not isinstance(title_, str) and not isinstance(Public_health_relevance_statement, str) and not isinstance(abstract_, str):
        return ''  # Return default values when no string input is provided
    if title_ and isinstance(title_, str):
        name = get_gard_title_stem_exact(title_)
        if name: return name

    if Public_health_relevance_statement and isinstance(Public_health_relevance_statement, str):
        A, B, C,D = check_sen(Public_health_relevance_statement)
        name1 = get_gard_abstract_stem_exact(A)
        name2 = get_gard_abstract_stem_exact(B)
        name3 = get_gard_abstract_stem_exact(C)
        name4 = get_gard_abstract_stem_exact(D)
        name=normalize_combined_dictionary(Public_health_relevance_statement,name1,name2,name3,name4,0.7,0.9)
        if name and (name !={}): return name

    if abstract_ and isinstance(abstract_, str):
        A, B, C , D = check_sen(abstract_)
        name1 = get_gard_abstract_stem_exact(A)
        name2 = get_gard_abstract_stem_exact(B)
        name3 = get_gard_abstract_stem_exact(C)
        name4 = get_gard_abstract_stem_exact(D)
        name=normalize_combined_dictionary(abstract_,name1,name2,name3,name4,0,0.7)
        if name and (name !={}): return name

        

def GardNameExtractor(project_title,phr_text,abstract_text):
  #Abstract1['Gard_name']=Abstract1.apply(lambda x: grad_id(x['project_title'],x['phr_text'],x['abstract_text']), axis=1)
  return  grad_id(project_title,phr_text,abstract_text)
