{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGeakebRaFwJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gard Name Preprocessing"
      ],
      "metadata": {
        "id": "vMCx1S37aIe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "Gard = pd.read_csv('/content/exporttttt.csv')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import json\n",
        "def extract_words_from_json_string(json_string):\n",
        "    try:\n",
        "        word_list = json.loads(json_string)\n",
        "        words = [word.replace('\"', '').strip() for word in word_list ]\n",
        "        return words\n",
        "    except (json.JSONDecodeError, TypeError) as e:\n",
        "        #print(f\"Error decoding JSON: {e}\")\n",
        "        return []\n",
        "\n",
        "Gard['GardName'] = Gard['GardName'].apply(lambda x: str(x).replace('\"', '').lower())\n",
        "Gard['Synonyms'] = Gard['Synonyms'].apply(lambda x: extract_words_from_json_string(str(x).lower()))\n",
        "\n",
        "import pandas as pd\n",
        "def remove_similar_strings(df):\n",
        "    for i in df.index:\n",
        "        if i % 2000 ==0 : print(i)\n",
        "        for j in df.index:\n",
        "            if i != j:\n",
        "                string_a = df['GardName'][i]\n",
        "                list_b = df['Synonyms'][j]\n",
        "                for item in list_b:  # Using [:] for iterating a copy of the list\n",
        "                    if item == string_a:\n",
        "                        list_b.remove(item)\n",
        "    return df\n",
        "\n",
        "Gard= remove_similar_strings(Gard)"
      ],
      "metadata": {
        "id": "NIea8qYKbSlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "def extract_words_from_json_string(input_string):\n",
        "    try:\n",
        "        # Use ast.literal_eval to safely convert the string to a list\n",
        "        result_list = ast.literal_eval(input_string)\n",
        "        if isinstance(result_list, list):\n",
        "            return result_list\n",
        "        else:\n",
        "            raise ValueError(\"Input is not a string representation of a list.\")\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        print(f\"Error converting string to list: {e}\")\n",
        "        return None\n",
        "\n",
        "def len_chcek(row):\n",
        "      return [w for w in row if (len(w) >3) and (w != 'plan') ]\n",
        "\n",
        "\n",
        "Gard = pd.read_csv('/content/Gard_V1.csv')\n",
        "Gard['Synonyms'] = Gard['Synonyms'].apply(lambda x: extract_words_from_json_string(x))\n",
        "Gard['Synonyms'] =Gard['GardName'].apply(lambda x: [x])+Gard['Synonyms']\n",
        "\n",
        "#######################          BOW       ########################################################################\n",
        "from itertools import permutations\n",
        "def generate_term_orders(terms):\n",
        "    words = terms.split()\n",
        "    if len(words) ==2:\n",
        "      all_permutations = list(permutations(words))\n",
        "      orders = [' '.join(permutation) for permutation in all_permutations]\n",
        "      return orders\n",
        "    else: return [terms]\n",
        "\n",
        "def generate_term_orders_list_of_sords(words):\n",
        "    X=[]\n",
        "    for i in words:\n",
        "      X+=generate_term_orders(i)\n",
        "    return X\n",
        "#Gard['Synonyms_bow']=Gard['Synonyms'].apply(lambda x: generate_term_orders_list_of_sords(x) )\n",
        "\n",
        "########################      Removing stop words  #########################################################\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the stop words dataset\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def process_row(row):\n",
        "    words = row.split()\n",
        "    if len(words) > 2 :\n",
        "        words = [word.lower()  for word in words if word.lower() not in ['syndrome','syndromes', 'disease','diseases']]\n",
        "    return ' '.join(words)\n",
        "def process_row_list(row):\n",
        "      return [process_row(w) for w in row]\n",
        "Gard['Synonyms_sw'] = Gard['Synonyms'].apply(lambda x: process_row_list(x))\n",
        "Gard['Synonyms_sw_bow']=Gard['Synonyms_sw'].apply(lambda x: generate_term_orders_list_of_sords(x) )\n",
        "Gard['Synonyms_sw_bow']=Gard['Synonyms_sw_bow'].apply(lambda x: list(set(len_chcek(x))) )\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "def process_row_list_2(row):\n",
        "    return [remove_stop_words(w) if (remove_stop_words(w) != '' and len(w.split()) > 2) else w for w in row]\n",
        "\n",
        "#Gard['Synonyms_sw_nltk'] = Gard['Synonyms_sw'].apply(lambda x: process_row_list_2(x))\n",
        "#Gard['Synonyms_sw_nltk']=Gard['Synonyms_sw_nltk']+Gard['Synonyms_sw']\n",
        "#Gard['Synonyms_sw_nltk'] = Gard['Synonyms_sw_nltk'].apply(lambda x: list(set(x)))\n",
        "\n",
        "########################      Text stemming  #########################################################\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "def stem_text(text):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    # Remove punctuation\n",
        "    text_without_punctuation = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text_without_punctuation)\n",
        "    # Perform stemming on each word\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    # Join the stemmed words back into a single string\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "def stem_text_list(row):\n",
        "      return [stem_text(w) for w in row if len(stem_text(w)) >2 ]\n",
        "\n",
        "#Gard['Synonyms_stem'] = Gard['Synonyms'].apply(lambda x: stem_text_list(x))\n",
        "#Gard['Synonyms_stem_bow']=Gard['Synonyms_stem'].apply(lambda x: generate_term_orders_list_of_sords(x) )\n",
        "Gard['Synonyms_sw_stem'] = Gard['Synonyms_sw'].apply(lambda x: stem_text_list(x))\n",
        "Gard['Synonyms_sw_stem_bow']=Gard['Synonyms_sw_stem'].apply(lambda x: generate_term_orders_list_of_sords(x) )\n",
        "Gard['Synonyms_sw_stem'] = Gard['Synonyms_sw'].apply(lambda x:list(set(len_chcek(x))) )\n",
        "Gard['Synonyms_sw_stem_bow']=Gard['Synonyms_sw_stem_bow'].apply(lambda x: list(set(len_chcek(x))) )\n",
        "\n",
        "Gard['Synonyms_sw'] = Gard['Synonyms'].apply(lambda x: list(set(len_chcek(x))) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKhnsUHhaLkF",
        "outputId": "129c0b06-8a0d-4031-9007-5f273f58d0cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RareDiseaseAlg"
      ],
      "metadata": {
        "id": "AXLmdZvdbwn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gard.to_csv('Gard_V1.csv', index=False)\n",
        "#Abstract = pd.read_csv('/content/Abstract_v5 (1).csv')#/content/abstract.csv')\n",
        "Abstract = pd.read_csv('/content/abstract.csv')#/content/abstract.csv')\n"
      ],
      "metadata": {
        "id": "LTTJge7Ta_PW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentence(sentence):\n",
        "    # Use regular expression to split words without including punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "    return words\n",
        "def word_matching(text,word):\n",
        "   for i in  split_sentence(word):\n",
        "     if i not in text:\n",
        "        return False\n",
        "   return True\n",
        "\n",
        "def get_gard_title(text, list_chcek):\n",
        "  if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']: text1=stem_text(text.lower())\n",
        "  elif list_chcek in [ 'Synonyms_sw_nltk']  :          text1=remove_stop_words(text.lower())\n",
        "  else:                                                  text1=text.lower()\n",
        "  text2=split_sentence(text1)\n",
        "  out=dict()\n",
        "  for i in Gard.index:\n",
        "    if Gard[list_chcek][i] != []:\n",
        "      for j in  Gard[list_chcek][i]:\n",
        "         if j in text1 and word_matching(text2,j)==True:\n",
        "           if Gard['GardName'][i] in out:\n",
        "                if len(j.split()) ==1:   out[Gard['GardName'][i]][0]+=text2.count(j)\n",
        "                else: out[Gard['GardName'][i]][0]+=text1.count(j)\n",
        "           else:\n",
        "                if len(j.split()) ==1:out[Gard['GardName'][i]]=[text2.count(j)]\n",
        "                else:  out[Gard['GardName'][i]]=[text1.count(j)]\n",
        "  if out== {}: return None\n",
        "  return out\n",
        "\n",
        "def get_gard_title_stem_exact(text):\n",
        "    exact_matching=get_gard_title(text, 'Synonyms_sw_bow')\n",
        "    #print(exact_matching)\n",
        "    Stemming_chcek=get_gard_title(text, 'Synonyms_sw_stem_bow')\n",
        "    #print(Stemming_chcek)\n",
        "    if exact_matching is None:\n",
        "        exact_matching = {}\n",
        "    if Stemming_chcek is None:\n",
        "        Stemming_chcek = {}\n",
        "    combined_dict = {}\n",
        "    combined_dict.update(exact_matching)\n",
        "    combined_dict.update(Stemming_chcek)\n",
        "    # Remove keys that are part of another key\n",
        "    keys_to_remove = set()\n",
        "    for key1 in combined_dict:\n",
        "        for key2 in combined_dict:\n",
        "            if key1 != key2 and key1 in key2:\n",
        "                keys_to_remove.add(key1)\n",
        "    for key in keys_to_remove:\n",
        "        del combined_dict[key]\n",
        "    if combined_dict=={}:return None\n",
        "    for key1 in combined_dict:\n",
        "        combined_dict[key1]=1\n",
        "    return combined_dict"
      ],
      "metadata": {
        "id": "cC7g-SL4hRfv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gard_title_stem_exact('RECOMBINANT DNA STRATEGIES--DUCHENNE MUSCULAR DYSTROPHY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3X13m3ZiHBl",
        "outputId": "bc4d9250-e427-494d-c608-180795e64b92"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'duchenne muscular dystrophy': [1]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Abstract1['Gard_name_title_finalized']=Abstract1.apply(lambda x: get_gard_title_stem_exact(x['project_title']), axis=1)"
      ],
      "metadata": {
        "id": "8idWnyJWiGGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load spaCy model with sentencizer component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Function to determine verb tense\n",
        "def get_verb_tense(verb):\n",
        "    if \"VBD\" in verb.tag_:\n",
        "        return \"past\"\n",
        "    elif (\"MD\" in verb.tag_ and \"will\" in verb.lemma_.lower()) or ('aim' in verb.lemma_.lower() ) :\n",
        "        return \"future\"\n",
        "    elif \"VBP\" in verb.tag_ or \"VBZ\" in verb.tag_:\n",
        "        return \"present\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "# Function to determine if a sentence is negated\n",
        "def is_sentence_negated(sentence):\n",
        "    for token in sentence:\n",
        "        if token.dep_ == \"neg\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def check_sen(text):\n",
        "  # Process the text\n",
        "  doc = nlp(text)\n",
        "  # Iterate over sentences in the document\n",
        "  Priority,Future_positive,present_positive,positive='','','',''\n",
        "  for i, sent in enumerate(doc.sents, 1):\n",
        "    # Initialize a set to store unique tenses in the sentence\n",
        "    sentence_tenses = set()\n",
        "    # Iterate over tokens in the sentence\n",
        "    for token in sent:\n",
        "        # Check if the token is a verb\n",
        "        if token.pos == spacy.symbols.VERB or token.pos == spacy.symbols.AUX:\n",
        "            # Check the tense of the verb\n",
        "            tense = get_verb_tense(token)\n",
        "            sentence_tenses.add(tense)\n",
        "\n",
        "    # Determine the overall tense of the sentence\n",
        "    if is_sentence_negated(sent)==False and  (\"past\" not in sentence_tenses):\n",
        "        #positive+=sent.text\n",
        "        if  (\"the goal of\" in sent.text.lower()) or (\"aim\" in sent.text.lower()):\n",
        "           Priority+=sent.text\n",
        "        elif \"future\" in sentence_tenses:\n",
        "           Future_positive+=sent.text\n",
        "        elif \"present\" in sentence_tenses and is_sentence_negated(sent)==False:\n",
        "           present_positive+=sent.text\n",
        "  return Priority,Future_positive,present_positive #,\n",
        "# Sample text\n",
        "text = \"The goal of tis project was ird. This aim is not to go the first sentence. This is not the second sentence? And this is the third sentence.\"\n",
        "check_sen(text)\n",
        "\n",
        "def get_sentence_with_word(paragraph, target_word):\n",
        "    if not isinstance(paragraph, str):\n",
        "        return ''\n",
        "\n",
        "    # Define characters indicating the start of a new sentence\n",
        "    new_sentence_chars = ['-', ':', ';', '1)', '2)', '3)', '4)', '5)', '6)', '7)', '8)']\n",
        "\n",
        "    # Split the paragraph into sentences using provided characters\n",
        "    for char in new_sentence_chars:\n",
        "        paragraph = paragraph.replace(char, '.')\n",
        "\n",
        "    # Split the paragraph into sentences using standard punctuation\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', paragraph)\n",
        "\n",
        "    # Check for the target word in each sentence\n",
        "    sen=''\n",
        "    for sentence in sentences:\n",
        "        if target_word.lower() in sentence.lower():\n",
        "            sen+= sentence\n",
        "\n",
        "    return sen\n",
        "def stem_text_finding(text):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    # Remove punctuation\n",
        "    text_without_punctuation = text\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text_without_punctuation)\n",
        "    # Perform stemming on each word\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    # Join the stemmed words back into a single string\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "def split_sentence(sentence):\n",
        "    # Use regular expression to split words without including punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "    return words\n",
        "def word_matching(text,word):\n",
        "   for i in  split_sentence(word):\n",
        "     if i not in text:\n",
        "        return False\n",
        "   return True\n",
        "\n",
        "def get_gard_abstract(text, list_chcek):\n",
        "  #text=check_sen(text)\n",
        "  if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']: text1=stem_text(text.lower())\n",
        "  elif list_chcek in [ 'Synonyms_sw_nltk']  :          text1=remove_stop_words(text.lower())\n",
        "  else:                                                  text1=text.lower()\n",
        "  text2=split_sentence(text1)\n",
        "  out=dict()\n",
        "  sen=dict()\n",
        "  for i in Gard.index:\n",
        "    if Gard[list_chcek][i] != []:\n",
        "      for j in  Gard[list_chcek][i]:\n",
        "         if j in text1 and word_matching(text2,j)==True:\n",
        "           if Gard['GardName'][i] in out:\n",
        "                if len(j.split()) ==1:\n",
        "                   out[Gard['GardName'][i]][0]+=text2.count(j)\n",
        "                   if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] += get_sentence_with_word(stem_text_finding(text.lower()), j)\n",
        "                   else:    sen[Gard['GardName'][i]] += get_sentence_with_word(text1, j)\n",
        "                else:\n",
        "                   out[Gard['GardName'][i]][0]+=text1.count(j)\n",
        "                   if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] += get_sentence_with_word(stem_text_finding(text.lower()), j)\n",
        "                   else:    sen[Gard['GardName'][i]] += get_sentence_with_word(text1, j)\n",
        "           else:\n",
        "                if len(j.split()) ==1:\n",
        "                     out[Gard['GardName'][i]]=[text2.count(j)]\n",
        "                     if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] = get_sentence_with_word(stem_text_finding(text.lower()), j)\n",
        "                     else:    sen[Gard['GardName'][i]] = get_sentence_with_word(text1, j)\n",
        "                else:\n",
        "                     out[Gard['GardName'][i]]=[text1.count(j)]\n",
        "                     if list_chcek in ['Synonyms_stem','Synonyms_sw_stem','Synonyms_stem_bow','Synonyms_sw_stem_bow']:sen[Gard['GardName'][i]] = get_sentence_with_word(stem_text_finding(text.lower()), j)\n",
        "                     else:    sen[Gard['GardName'][i]] = get_sentence_with_word(text1, j)\n",
        "  if out== {}: return None,None\n",
        "  return out,sen"
      ],
      "metadata": {
        "id": "3z9fSulbjOI7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_dictionaries_count(dict1, dict2):\n",
        "    combined_dict = {}\n",
        "    # Update combined_dict with values from dict1\n",
        "    for key, value in dict1.items():\n",
        "        combined_dict[key] = combined_dict.get(key, 0) + sum(value)\n",
        "    # Update combined_dict with values from dict2\n",
        "    for key, value in dict2.items():\n",
        "        combined_dict[key] = combined_dict.get(key, 0) + sum(value)\n",
        "    return combined_dict\n",
        "\n",
        "def combine_dictionaries_sent(dict1, dict2):\n",
        "    combined_dict = {}\n",
        "    # Update combined_dict with values from dict1\n",
        "    for key, value in dict1.items():\n",
        "        if key in combined_dict:\n",
        "            combined_dict[key] += value\n",
        "        else:\n",
        "            combined_dict[key] = value\n",
        "    # Update combined_dict with values from dict2\n",
        "    for key, value in dict2.items():\n",
        "        if key in combined_dict:\n",
        "            combined_dict[key] += value\n",
        "        else:\n",
        "            combined_dict[key] = value\n",
        "    return combined_dict\n",
        "\n",
        "def  modified_dict(combined_dict,combined_dict_sen):\n",
        "    keys_to_remove = set()\n",
        "    for key1 in combined_dict:\n",
        "        for key2 in combined_dict:\n",
        "          #try:\n",
        "            if key1 != key2 and (key1 in key2) and (combined_dict[key1] <= combined_dict[key2]) and (combined_dict_sen[key1] in combined_dict_sen[key2]):\n",
        "                keys_to_remove.add(key1)\n",
        "          #except:\n",
        "          #  pass\n",
        "    for key in keys_to_remove:\n",
        "        del combined_dict[key]\n",
        "        del combined_dict_sen[key]\n",
        "    return combined_dict\n",
        "\n",
        "\n",
        "def get_gard_abstract_stem_exact(text):\n",
        "  if text and isinstance(text, str):\n",
        "    exact_matching, exact_matching_sen=get_gard_abstract(text, 'Synonyms_sw')\n",
        "    #print(exact_matching)\n",
        "    Stemming_chcek, Stemming_chcek_sen=get_gard_abstract(text, 'Synonyms_sw_stem')\n",
        "    #print(Stemming_chcek)\n",
        "    if exact_matching is None:exact_matching = {}\n",
        "    if Stemming_chcek is None:Stemming_chcek = {}\n",
        "    if exact_matching_sen is None:exact_matching_sen = {}\n",
        "    if Stemming_chcek_sen is None:Stemming_chcek_sen = {}\n",
        "\n",
        "    combined_dict    = combine_dictionaries_count(exact_matching,Stemming_chcek)\n",
        "    combined_dict_sen= combine_dictionaries_sent(exact_matching_sen,Stemming_chcek_sen)\n",
        "    # Remove keys that are part of another key\n",
        "    combined_dict=modified_dict(combined_dict,combined_dict_sen)\n",
        "    if combined_dict=={}:return {}\n",
        "    return combined_dict\n",
        "  return {}"
      ],
      "metadata": {
        "id": "jh5TwB64jtQD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_combined_dictionary(dict1, dict2, dict3, min_, max_):\n",
        "    # Make the values of the first dictionary three times\n",
        "    dict1 = {key: value * 3 for key, value in dict1.items()}\n",
        "    # Make the values of the second dictionary two times\n",
        "    dict2 = {key: value * 2 for key, value in dict2.items()}\n",
        "    # Combine all dictionaries\n",
        "    combined_dict = {key: dict1.get(key, 0) + dict2.get(key, 0) + dict3.get(key, 0) for key in set(dict1) | set(dict2) | set(dict3)}\n",
        "    # Normalize the values of the combined dictionary\n",
        "    total_frequency = sum(combined_dict.values())\n",
        "\n",
        "    # Check if total_frequency is zero to avoid division by zero\n",
        "    if total_frequency == 0:\n",
        "        return {}\n",
        "    normalized_dict = {key: min_ + (max_ - min_) * (value / total_frequency) for key, value in combined_dict.items()}\n",
        "    return normalized_dict\n",
        "\n",
        "\n",
        "def grad_id(title_, Public_health_relevance_statement, abstract_):\n",
        "    if not isinstance(title_, str) and not isinstance(Public_health_relevance_statement, str) and not isinstance(abstract_, str):\n",
        "        return ''  # Return default values when no string input is provided\n",
        "    if title_ and isinstance(title_, str):\n",
        "        name = get_gard_title_stem_exact(title_)\n",
        "        if name: return name\n",
        "\n",
        "    if Public_health_relevance_statement and isinstance(Public_health_relevance_statement, str):\n",
        "        A, B, C = check_sen(Public_health_relevance_statement)\n",
        "        name1 = get_gard_abstract_stem_exact(A)\n",
        "        name2 = get_gard_abstract_stem_exact(B)\n",
        "        name3 = get_gard_abstract_stem_exact(C)\n",
        "        name=normalize_combined_dictionary(name1,name2,name3,0.7,0.9)\n",
        "        if name and (name !={}): return name\n",
        "\n",
        "    if abstract_ and isinstance(abstract_, str):\n",
        "        A, B, C = check_sen(abstract_)\n",
        "        name1 = get_gard_abstract_stem_exact(A)\n",
        "        name2 = get_gard_abstract_stem_exact(B)\n",
        "        name3 = get_gard_abstract_stem_exact(C)\n",
        "        name=normalize_combined_dictionary(name1,name2,name3,0,0.7)\n",
        "        if name and (name !={}): return name\n",
        "Abstract1['Gard_name']=Abstract1.apply(lambda x: grad_id(x['project_title'],x['phr_text'],x['abstract_text']), axis=1)\n"
      ],
      "metadata": {
        "id": "BfkBSD0XlBJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Abstract1.to_csv('check_.csv', index=False)"
      ],
      "metadata": {
        "id": "2M36V1DXw_ZB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Abstract = pd.read_csv('/content/Sample_2.csv')#/content/abstract.csv')\n",
        "Abstract1=Abstract[:]\n",
        "Abstract1.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ggzk3qAvhbq",
        "outputId": "ea433211-4ad3-4ab4-ab9d-c4649e15c1d1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['abstract_text', 'project_title', 'phr_text',\n",
              "       'Gard_name_statement_finalized', 'Sentence_statement_finalized',\n",
              "       'Gard_name_abstract_finalized', 'Sentence_finalized',\n",
              "       'Gard_name_title_finalized'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Abstract1['Gard_name_statement_finalized_2'] = Abstract1.apply(lambda x: pd.Series(get_gard_abstract_stem_exact( x['abstract'])), axis=1)\n"
      ],
      "metadata": {
        "id": "yoru-o-tjxS_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}